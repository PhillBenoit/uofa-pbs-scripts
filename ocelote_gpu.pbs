#!/bin/bash

### This will be the researcher who supported your use of the HPC.
#PBS -W group_list=[your researcher here]

### Researchers can buy in for priority queueing. However, the amount of time 
### they can use for this is limited.  Everyone has access to unlimited
### windfall.  However, any priority jobs will go first.  The queues on Ocelote
### GPU nodes are windfall and standard
#PBS -q [your queue here]

### GPU nodes have 28 cores of standard processors.  When requesting a GPU node,
### it is important to request the whole node.  Each GPU node request will have
### the configuration below.  If you need more nodes, simply increase the
### "select" number.  This shows how to request 1 GPU.  Keep in mind that there
### is only 1 GPU per node on the system.  If you want more than 1 GPU, you
### will be requesting multiple nodes.  There are 45 nodes but requesting that
### many is not recommended because they will not all be available at once and
### your job is likely to get stuck in the queue.  With GPU requests, it's
### important to also configure your software to recognize the GPU resources.
### This will also inform how many you request.  If you need help, please reach
### out to the HPC consult team.
#PBS -l select=1:ncpus=28:mem=250gb:pcmem=8gb:np100s=1

### This is the amount of time you think your job will take to run.
### Not sure how long it will take?  Request the max of 10 days.
### (240:00:00)                  #PBS -l walltime=240:00:00
### This request shows 5 minutes #PBS -l walltime=00:05:00
#PBS -l walltime=[your time here]

### This will show which node your job is running on.
echo 'This script is running on:'
hostname

### GPU test program
nvidia-smi

### If you have any trouble or questions, please contact:
### hpc-consult@list.arizona.edu
